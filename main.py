import requestsimport randomimport datetimeimport chromadbimport re ## Pytyhon Modul f체r regul채re Ausdr체ckefrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunctionembedder = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")####### Ollama HelperOLLAMA_HOST = "http://localhost:11434"MODEL_NAME = "llama3:8b"def ollama_generate(prompt, max_tokens=500):    r = requests.post(        f"{OLLAMA_HOST}/api/generate",        json={"model": MODEL_NAME, "prompt": prompt, "stream": False},        timeout=120,    )    r.raise_for_status()    return (r.json().get("response") or "").strip()####### saving datas in our specific file (needs possibly to be changed to run code)chroma_client = chromadb.PersistentClient(path="./chroma_db")collection = chroma_client.get_or_create_collection(    name="my_collection",    embedding_function=embedder)members = ['Elias', 'Leon', 'Matteo']# Initialize memory stream (as a list first)conversation_history = []# topic (any topic can be chosen)topic = 'project for ai integrated self driving cars'num_rounds = 5def generate_rating_prompt(conversation_history, members):    conversation_text = "\n".join(conversation_history)    prompt = f"Rate the contributions in the following conversation and provide an explanation for each rating:\n{conversation_text}\n\n"    for member in members:        prompt += f"Rate {member}'s overall contributions on a scale of 1 (good) to 10 (bad) and provide an explanation: "    return promptdef get_member_ratings(conversation_history, members):    prompt = generate_rating_prompt(conversation_history, members)    ratings_text = ollama_generate(prompt)    ratings = {}    for member in members:        # search for "member: Rating - Explanation"        match = re.search(f"{member}: (\\d+|No rating found) - (.*?)$", ratings_text, re.MULTILINE)        if match:            rating = match.group(1)            explanation = match.group(2).strip()        else:            rating = "No rating found"            explanation = "No explanation provided"        ratings[member] = (rating, explanation)    return ratingsdef generate_explanation_prompt(members, ratings):    prompt = "Please explain why the following ratings were given:\n"    for member in members:        prompt += f"{member}: {ratings[member]} - Explanation: "    return promptdef get_explanations_for_ratings(members, ratings):    prompt = generate_explanation_prompt(members, ratings)    explanations_text = ollama_generate(prompt)    explanations = {}    for line in explanations_text.split('\n'):        parts = line.split('-')        if len(parts) == 2 and parts[0].strip() in members:            explanations[parts[0].strip()] = parts[1].strip()    return explanations# generating high level questions based on conversation_summaries (for each person)def generate_aspects_reflection(conversation_summaries):    prompt = "Given only the information here {conversation_summaries}. What are the 3 most high-level questions of the subjects in the statements?"    for summary in conversation_summaries:        prompt += f"\n{summary['participant']}: {summary['conversation']}"    prompt += "\n\nQuestions:"    return ollama_generate(prompt)# using the generated high-level reflections to generate answers (statements) by conversation_summaeriesdef generate_reflection_prompt(conversation_summaries):    # Generieren von Aspekten basierend auf den Gespr채chszusammenfassungen bzgl. der Fragen    aspects_reflection = generate_aspects_reflection(conversation_summaries)    # Erstellen eines Prompts, das die Reflexionen der Teilnehmer und die Gruppendynamik umfasst    prompt = "Based on the following aspects reflections, provide deeper reflections about each chat participant:\n\n"    prompt += aspects_reflection    prompt += "\n\nDeeper Reflections:"    return promptdef get_reflections(conversation_summaries):    prompt = generate_reflection_prompt(conversation_summaries)    return ollama_generate(prompt)# Function that simulates the conversation for a memberdef conduct_conversation_for_member(member):    # Here your existing code for conversation simulation would come    print(f"Conversation simulation for {member}")# Main function that is executed dailydef daily_conversation_decision():    today = datetime.date.today()    participating_members = members.copy()  # Create a copy of the member list    for member in participating_members:        print(f"Today, on {today}, {member} should have a conversation.")        conduct_conversation_for_member(member)    for member in set(members) - set(participating_members):        print(f"Today, on {today}, {member} should not have a conversation.")def generate_summary_prompt(conversation_history):    conversation_text = "\n".join(conversation_history)    prompt = f"Summary of the following conversation:\n{conversation_text}\n\nSummary:"    return promptdef get_conversation_summary(conversation_history):    prompt = generate_summary_prompt(conversation_history)    summary = ollama_generate(prompt, max_tokens=500)    return summary# generates suggestions to improve responses based on high-level-insights (get_reflections)def generate_improvement_suggestions_with_ai(conversation_summaries):    # running the method get_reflections() and further    reflection = get_reflections(conversation_summaries)    # generate suggestions    prompt = "Generate improvement suggestions for the responses of each person that participated:\n\n" + reflection    # using system as role to later on add it to the prompts in line 208    # if that does not work it will get anyway added later on in conversation_summary and in the database    improvement_suggestions = ollama_generate(prompt, max_tokens=800)    print("suggestions: ")    print(prompt)    return improvement_suggestionsdef limit_response_length(response, max_length=10000):    return response if len(response) <= max_length else response[:max_length] + "..."if __name__ == "__main__":    # run daily decision method    daily_conversation_decision()# Read system description from filewith open('prompt_templates/system_desc', "r") as f:    system_prompt = f.read()# Replace placeholders in system prompt with actual topic and number of roundssystem_prompt = system_prompt.replace("<topic>", topic)system_prompt = system_prompt.replace("<Num_Rounds>", str(num_rounds))system_prompt = system_prompt.replace("<members>", str(members))# Read the persona description from a filewith open('prompt_templates/persona_desc', "r") as f:    persona_prompt = f.read()# Read specific instructions for Person Awith open('prompt_templates/instructions/Elias/v1', "r") as f:    instruction_personA = f.read()# Read specific instructions for Person Bwith open('prompt_templates/instructions/Leon/v1', "r") as f:    instruction_personB = f.read()# Read specific instructions for Person Cwith open('prompt_templates/instructions/Matteo/v1', "r") as f:    instruction_personC = f.read()# Read the old Conversations if existexisting = collection.get(include=["documents"])if existing and existing.get("documents"):    # Falls Chroma eine Liste von Strings liefert    conversation_history.extend(existing["documents"])# Create prompts for Person A, Person B and Person C by combining the persona description and specific instructionspersonA_prompt = persona_prompt.replace('<Role>', 'Elias').replace('<Instruction>', instruction_personA)personB_prompt = persona_prompt.replace('<Role>', 'Leon').replace('<Instruction>', instruction_personB)personC_prompt = persona_prompt.replace('<Role>', 'Matteo').replace('<Instruction>', instruction_personC)# Add the system prompt to the prompts for Person A, Person B and Person CpersonA_prompt = system_prompt + '\n' + personA_prompt + '\n'personB_prompt = system_prompt + '\n' + personB_prompt + '\n'personC_prompt = system_prompt + '\n' + personC_prompt + '\n'round = 1while round <= num_rounds:    # Update the prompts with the response from Person A    responseA = ollama_generate(        personA_prompt + '\n you respond to the last response of Leon or PersonC. After generating, it should display the topic: [topic]; Rounds to go before conversation ends: ' + str(num_rounds - round)    )    personA_prompt = personA_prompt + '\n\n' + responseA    personB_prompt = personB_prompt + '\n\n' + responseA    personC_prompt = personC_prompt + '\n\n' + responseA    # add response A to provisionally memory stream    conversation_history.append(f"PersonA: {responseA}")    # Output the response from Person A    print(f"Round {round} - PersonA's response:")    print(responseA)    # Update the prompts with the response from Person B    responseB = ollama_generate(        personB_prompt + '\n If you start the conversation, generate a statement once of the topic.  On the other hand, youre based on the statement of Elias which points out a positive aspect about the topic that also invalidates the statement of Elias or PersonC. Rounds to go before conversation ends: ' + str(num_rounds - round)    )    personA_prompt = personA_prompt + '\n\n' + responseB    personB_prompt = personB_prompt + '\n\n' + responseB    personC_prompt = personC_prompt + '\n\n' + responseB    # add response B to provisionally memory stream    conversation_history.append(f"PersonB: {responseB}")    # Output the response from Person B    print(f"Round {round} - PersonB's response:")    print(responseB)    # Update the prompts with the response from Person C    responseC = ollama_generate(        personC_prompt + '\n If you start the conversation, generate a statement once of the topic.  On the other hand, youre based on the conversation of personA or Leon so far. Rounds to go before conversation ends: ' + str(num_rounds - round)    )    personA_prompt = personA_prompt + '\n\n' + responseC    personB_prompt = personB_prompt + '\n\n' + responseC    personC_prompt = personC_prompt + '\n\n' + responseC    # add response C to provisionally memory stream    conversation_history.append(f"PersonC: {responseC}")    # Output the response from Person B    print(f"Round {round} - PersonC's response:")    print(responseC)    # Request user feedback and integrate it into prompts (nicht ganz sicher, sometimes interrupts the flow and sometimes not)    #user_feedback = input("Please provide feedback for the conversation: ")    personA_prompt += f'\n\nUser Feedback for Round {round}'    personB_prompt += f'\n\nUser Feedback for Round {round}'    personC_prompt += f'\n\nUser Feedback for Round {round}'    # personC_prompt += f'\n\nUser Feedback for Round {round}: {user_feedback}'    # Proceed to the next round    round += 1# After all conversation rounds are completedsummary = get_conversation_summary(conversation_history)ratings = get_member_ratings(conversation_history, members)# Prepare data for reflectionsconversation_summaries = [{"participant": member, "conversation": summary} for member in members]# Generate reflectionsreflections = get_reflections(conversation_summaries)# generate suggestions out of reflectionssuggestions = generate_improvement_suggestions_with_ai(conversation_summaries)# add reflection to conversation_history (provisionally memory stream)conversation_history.append(f"Reflections: {reflections}")# add suggestion to provisionally memory streamconversation_history.append(f"Suggestions: {suggestions}")# Display summary, ratings, reflections and suggestionprint("\nSummary:\n", summary)print("\nRatings of overall contributions with explanations:")for member, (rating, explanation) in ratings.items():    if rating is not None:        print(f"{member}: {rating} - {explanation}")    else:        print(f"{member}: No rating available")print("\nReflections on the conversations:")print(reflections)print("\nSuggestions based on the Reflections and further more:")print(suggestions)# Wir sind uns nicht sicher ob vor oder nachher besser ist, i guess better than nothingconversation_history.append(f"Reflections: {reflections}")# Request of database (specifically)results = collection.query(query_texts=["flashcards"], n_results=10)# iterate trough outer listfor inner_list in results["documents"]:    # iterate trough every document in inner list    for document in inner_list:        print(document)from datetime import datetime# Eindeutige Session-ID basierend auf Datum/Uhrzeitrun_id = datetime.now().strftime("%Y%m%d%H%M%S")# After completing all conversation rounds (adding to database)if conversation_history:    try:        for index, entry in enumerate(conversation_history):            document_id = f"conversation_entry_{run_id}_{index}"            collection.add(documents=[entry], ids=[document_id])    except Exception as e:        print(f"Error adding the entry to the database: {e}")